# Qwen3-Coder RunPod Ollama API

Deploy Qwen3-Coder (30B/480B) on RunPod GPU with Ollama and LiteLLM proxy. Complete setup with secure OpenAI-compatible API endpoint, authentication, persistent storage, automated backups, and VS Code integration.

## ğŸš€ Features
- Secure OpenAI-compatible API endpoint
- API key authentication
- Persistent storage configuration
- Automated backup system
- VS Code integration ready
- Multi-model support (30B/480B)
- Production-ready deployment

## ğŸ› ï¸ Requirements
- RunPod account with GPU instance
- Docker (optional but recommended)
- VS Code with Cline extension

## ğŸ“ Project Structure
```
runpod/
â”œâ”€â”€ README.md
â”œâ”€â”€ VSCODE_INTEGRATION.md
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ start_ollama.sh
â”‚   â”œâ”€â”€ backup_config.sh
â”‚   â””â”€â”€ litellm-config.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ model_management.sh
â”‚   â””â”€â”€ health_check.sh
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ docker-compose.yml
â””â”€â”€ docs/
    â”œâ”€â”€ TROUBLESHOOTING.md
    â””â”€â”€ PERFORMANCE_TUNING.md
```

## ğŸ“‹ Setup Instructions
1. Create RunPod instance with PyTorch template
2. Install Ollama and required dependencies
3. Pull Qwen3-Coder model
4. Configure LiteLLM proxy with authentication
5. Start services with deployment scripts
6. Configure VS Code integration

## ğŸ¯ Usage
- API Endpoint: `https://your-runpod-endpoint.runpod.io/v1`
- Model Names: `qwen3-coder`, `gpt-3.5-turbo`, `gpt-4`
- Authentication: Bearer token required